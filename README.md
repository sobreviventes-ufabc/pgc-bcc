# ü§ñ Chatbot Assistente UFABC (RAG)

Chatbot para a UFABC baseado em **Retrieval-Augmented Generation (RAG)**. Ele busca trechos em documentos institucionais (PDFs, tabelas e imagens), sumariza e usa um LLM para responder com contexto.

## ‚ú® Principais features

- **RAG multimodal**: texto, tabelas (HTML) e imagens (sumarizadas na indexa√ß√£o)
- **Persist√™ncia**: vetores em **Chroma** e **docstore** em disco (LocalFileStore)
- **Reidrata√ß√£o inteligente**: se o docstore sumir, ele √© reconstru√≠do **sem re-embedar**
- **API FastAPI** (concorr√™ncia pronta) + **CLI** (modo terminal)
- **Ollama** para embeddings locais (fallbacks de LLM: Groq/OpenAI, se configurados)
- **Classifica√ß√£o robusta** em `parse_docs` (evita confundir texto com base64)

## üóÇÔ∏è Estrutura do projeto

```
rag_pipeline/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ api.py                     # FastAPI (servi√ßo)
‚îú‚îÄ‚îÄ main.py                    # CLI (terminal)
‚îú‚îÄ‚îÄ config.py                  # Paths absolutos e configs
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # get_llama_model / get_llava_model
‚îÇ   ‚îú‚îÄ‚îÄ prompt_utils.py        # parse_docs, build_prompt etc.
‚îÇ   ‚îî‚îÄ‚îÄ retriever_pipeline.py  # get_rag_pipeline (Op√ß√£o B com reidrata√ß√£o)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ pdf_utils.py           # extra√ß√£o (unstructured) e classifica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ summarization.py       # sumariza√ß√£o + add_documents (vectorstore + docstore)
‚îÇ   ‚îî‚îÄ‚îÄ retry.py               # retry_with_backoff
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ display_utils.py       # helper p/ exibir imagens base64 (CLI)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ .cache_chunks/             # gerado em runtime (chroma_store, summaries, chunks)
```

## üß© Requisitos

- Python **3.10+**
- [Ollama](https://ollama.com/download) (para embeddings locais)
- (Opcional) chaves **OpenAI** / **Groq** para fallback do LLM

### Instala√ß√£o

```bash
python -m venv .venv
source .venv/bin/activate     # Linux/macOS
# .venv\Scripts\activate      # Windows

pip install -r requirements.txt
```

> Se o `unstructured.partition.pdf` pedir extras (OCR), instale variantes como `unstructured[all-docs]`.

## üîê Vari√°veis de ambiente

Crie um `.env` (ou exporte no shell):

```bash
# Fallbacks de LLM (opcional)
export OPENAI_API_KEY="sk-..."
export GROQ_API_KEY="gsk-..."

# Se o Ollama estiver remoto
export OLLAMA_HOST="http://<ip-ou-host>:11434"
```

## üì¶ Baixar os modelos no Ollama

Certifique-se de que o Ollama est√° rodando (ollama serve) e ent√£o baixe os modelos usados pelo projeto:
```bash
# LLM para gera√ß√£o de respostas
ollama pull llama3.1:8b

# LLM multimodal para sumarizar imagens na indexa√ß√£o
ollama pull llava:13b

# Modelo de embeddings (texto)
ollama pull nomic-embed-text

# Certifica que os modelos est√£o instalados
ollama list
```

No **Windows**, para expor o Ollama para a rede/WSL:

```powershell
$env:OLLAMA_HOST="0.0.0.0:11434"
$env:OLLAMA_ORIGINS="*"
ollama serve
# libere a porta 11434 no Firewall do Windows
```


Comando para iniciar a API:

```
uvicorn rag_pipeline.api:app --reload
```

Como fazer perguntas:

Pergunta unica:
```
curl --request POST \
  --url http://127.0.0.1:8000/ask \
  --header 'Content-Type: application/json' \
  --header 'User-Agent: insomnia/11.5.0' \
  --data '{
	"question": "quantas vezes posso trancar a matricula?"
}'
```

Modo Chat:
```
curl --request POST \
  --url http://127.0.0.1:8000/chat \
  --header 'Content-Type: application/json' \
  --header 'User-Agent: insomnia/11.5.0' \
  --data '{
	"messages": [
	{"role": "user", "content": "Me explique brevemente sobre a matricula"},	
			{"role": "system", "content": "Ol√°!"},
{"role": "user", "content": "Ol√°"}
	]
}'
```

## üó∫Ô∏è Configura√ß√£o de paths

Os paths s√£o absolutos (via `Path.resolve()`) a partir da raiz do reposit√≥rio:

- PDFs: `data_extraction/documentos_ufabc/Prograd`
- Cache de chunks: `.cache_chunks/chunks_classificados.json`
- Cache de summaries: `.cache_chunks/summaries.json`
- Vetores/Chroma + docstore: `.cache_chunks/chroma_store/`

# ğŸ¤– Chatbot Assistente UFABC

Este repositÃ³rio contÃ©m o desenvolvimento de um **chatbot assistente para a UFABC**, utilizando a arquitetura de **Retrieval-Augmented Generation (RAG)**. O objetivo do projeto Ã© facilitar o acesso a informaÃ§Ãµes institucionais, documentos e dÃºvidas frequentes da universidade de forma automatizada, eficiente e interativa.

## ğŸ’¡ Sobre o Projeto

O chatbot tem como propÃ³sito atender Ã  comunidade acadÃªmica da UFABC (Universidade Federal do ABC), oferecendo respostas contextualmente relevantes a partir de uma base de documentos institucionais, como editais, regulamentos e informaÃ§Ãµes acadÃªmicas.

A abordagem RAG combina tÃ©cnicas de recuperaÃ§Ã£o de documentos com geraÃ§Ã£o de linguagem natural, permitindo que o chatbot consulte documentos reais antes de formular respostas, garantindo maior precisÃ£o e confiabilidade.

## âš™ï¸ Tecnologias e Conceitos

- **Retrieval-Augmented Generation (RAG)**
- **Processamento de Linguagem Natural (PLN)**

## ğŸ‘¥ Autores

- **Aline Milene Martins dos Santos**  
  ğŸ“§ aline.milene@aluno.ufabc.edu.br  
  ğŸ”— [github.com/AlineMilene](https://github.com/AlineMilene)

- **Leonardo Pires de Oliveira**  
  ğŸ“§ oliveira.l@aluno.ufabc.edu.br  
  ğŸ”— [github.com/LeonOliveir4](https://github.com/LeonOliveir4)

- **Matheus Victor Soares de Araujo**  
  ğŸ“§ matheus.victor@aluno.ufabc.edu.br  
  ğŸ”— [github.com/MatheusR42](https://github.com/MatheusR42)

---

Este projeto faz parte do Projeto de GraduaÃ§Ã£o de Curso (PGC) na **Universidade Federal do ABC (UFABC)**.

## ğŸš€ Como executar o projeto localmente

1ï¸âƒ£ Crie e ative um ambiente virtual (opcional, mas recomendado):

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate      # Windows
```

2ï¸âƒ£ Instale as dependÃªncias:

```bash
pip install bm25s[full] fastapi uvicorn openai
uvicorn rag_pipeline.main:app --host 0.0.0.0 --port 8000 --reload
```

---

#### ğŸ”¹ OpÃ§Ã£o 1: vLLM (LLaMA 3.1)

```bash
pip install vllm
python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3.1-8B-Instruct
```

Configure o endpoint da LLM no arquivo `llama_client.py`:

```python
openai.base_url = "http://localhost:8000/v1"
```

##### ğŸ”‘ Acesso ao modelo LLaMA 3.1 (obrigatÃ³rio)

1ï¸âƒ£ Tenha uma conta no [Hugging Face](https://huggingface.co)  
2ï¸âƒ£ Solicite acesso:  
ğŸ‘‰ [meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)  
3ï¸âƒ£ ApÃ³s aprovaÃ§Ã£o, gere um Access Token em:  
ğŸ‘‰ [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)  
4ï¸âƒ£ FaÃ§a login local com o token:

```bash
pip install huggingface-hub
huggingface-cli login
```

#### â— Alternativa open-source:

```bash
python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2
```

---

#### ğŸ”¹ OpÃ§Ã£o 2: Ollama (mais leve)

1ï¸âƒ£ Instale o Ollama:  
ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

2ï¸âƒ£ Rode o modelo desejado (exemplo com LLaMA 3):

```bash
ollama run llama3:8b
# ou outro modelo:
ollama run mistral
```

3ï¸âƒ£ Configure o endpoint no `llama_client.py`:

```python
openai.api_key = "sk-no-key-needed"
openai.base_url = "http://localhost:11434/v1"
```

4ï¸âƒ£ Execute o servidor FastAPI normalmente:

```bash
uvicorn rag_pipeline.main:app --host 0.0.0.0 --port 8000 --reload
```

5ï¸âƒ£ Acesse a interface de testes:  
ğŸ‘‰ [http://localhost:8000/docs](http://localhost:8000/docs)

---

ğŸ“š Modelos suportados por Ollama (exemplos):

- `llama3:8b`
- `mistral`
- `codellama`
- `phi3`
- `gemma`

Veja mais em: [https://ollama.com/library](https://ollama.com/library)

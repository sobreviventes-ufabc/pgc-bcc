# ü§ñ Chatbot Assistente UFABC (RAG)

Chatbot para a UFABC baseado em **Retrieval-Augmented Generation (RAG)**. Ele busca trechos em documentos institucionais (PDFs, tabelas e imagens), sumariza e usa um LLM para responder com contexto.

## ‚ú® Principais features

- **RAG multimodal**: texto, tabelas (HTML) e imagens (sumarizadas na indexa√ß√£o)
- **Persist√™ncia**: vetores em **Chroma** e **docstore** em disco (LocalFileStore)
- **Reidrata√ß√£o inteligente**: se o docstore sumir, ele √© reconstru√≠do **sem re-embedar**
- **API FastAPI** (concorr√™ncia pronta) + **CLI** (modo terminal)
- **Ollama** para embeddings locais (fallbacks de LLM: Groq/OpenAI, se configurados)
- **Classifica√ß√£o robusta** em `parse_docs` (evita confundir texto com base64)

## üóÇÔ∏è Estrutura do projeto

```
rag_pipeline/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ api.py                     # FastAPI (servi√ßo)
‚îú‚îÄ‚îÄ main.py                    # CLI (terminal)
‚îú‚îÄ‚îÄ config.py                  # Paths absolutos e configs
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # get_llama_model / get_llava_model
‚îÇ   ‚îú‚îÄ‚îÄ prompt_utils.py        # parse_docs, build_prompt etc.
‚îÇ   ‚îî‚îÄ‚îÄ retriever_pipeline.py  # get_rag_pipeline (Op√ß√£o B com reidrata√ß√£o)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ pdf_utils.py           # extra√ß√£o (unstructured) e classifica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ summarization.py       # sumariza√ß√£o + add_documents (vectorstore + docstore)
‚îÇ   ‚îî‚îÄ‚îÄ retry.py               # retry_with_backoff
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ display_utils.py       # helper p/ exibir imagens base64 (CLI)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ .cache_chunks/             # gerado em runtime (chroma_store, summaries, chunks)
```

## üß© Requisitos

- Python **3.10+**
- [Ollama](https://ollama.com/download) (para embeddings locais)
- (Opcional) chaves **OpenAI** / **Groq** para fallback do LLM

### Instala√ß√£o

1. **Clone o reposit√≥rio e configure o ambiente Python**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate     # Linux/macOS
   # .venv\Scripts\activate      # Windows
   
   cd rag_pipeline

   pip install -r requirements.txt
   ```

2. **Configure as vari√°veis de ambiente**:
   ```bash
   cp .env.example .env
   # Edite o arquivo .env com suas chaves de API
   ```

3. **Verifique se o Ollama est√° rodando**:
   ```bash
   ollama serve
   ```

4. **Se for usar modelos locais, instale os modelos**:
   ```bash
   ollama pull llama3.2:latest
   ollama pull nomic-embed-text:latest
   ```

> **Nota**: Se o `unstructured.partition.pdf` pedir extras (OCR), instale variantes como `unstructured[all-docs]`.

## üîê Configura√ß√£o de Vari√°veis de Ambiente

### Setup para Desenvolvimento

1. **Copie o arquivo de exemplo**:
   ```bash
   cp .env.example .env
   ```

2. **Edite o arquivo `.env`** com suas chaves reais:
   ```bash
   # API Keys obrigat√≥rias
   GROQ_API_KEY=sua_chave_groq_aqui
   OPENAI_API_KEY=sua_chave_openai_aqui
   
   # Base URL do Ollama (padr√£o: localhost)
   OLLAMA_BASE_URL=http://localhost:11434
   
   # Provedor de modelo para gera√ß√£o de texto (padr√£o: ollama)
   # Op√ß√µes: ollama, openai, groq
   MODEL_PROVIDER=ollama
   ```

### Obtendo as API Keys

- **Groq**: Registre-se em [console.groq.com](https://console.groq.com) para obter sua chave gratuita
- **OpenAI**: Acesse [platform.openai.com/api-keys](https://platform.openai.com/api-keys) para gerar uma API key

### Configura√ß√£o para Produ√ß√£o/Servidor

Em ambiente de produ√ß√£o, defina as vari√°veis de ambiente diretamente no sistema:

```bash
# Linux/macOS
export GROQ_API_KEY="sua_chave_groq"
export OPENAI_API_KEY="sua_chave_openai"
export OLLAMA_BASE_URL="http://seu-servidor-ollama:11434"
export MODEL_PROVIDER="ollama"

# Windows
set GROQ_API_KEY=sua_chave_groq
set OPENAI_API_KEY=sua_chave_openai
set OLLAMA_BASE_URL=http://seu-servidor-ollama:11434
set MODEL_PROVIDER=ollama
```

### Valida√ß√£o das Vari√°veis

O sistema valida automaticamente se todas as vari√°veis obrigat√≥rias est√£o configuradas:
- `GROQ_API_KEY`
- `OPENAI_API_KEY` 
- `OLLAMA_BASE_URL`
- `MODEL_PROVIDER` (opcional, padr√£o: "ollama")

Se alguma vari√°vel estiver faltando, voc√™ ver√° um erro como:
```
ValueError: Missing required environment variables: GROQ_API_KEY, OLLAMA_BASE_URL
```

## üì¶ Baixar os modelos no Ollama

Certifique-se de que o Ollama est√° rodando (ollama serve) e ent√£o baixe os modelos usados pelo projeto:
```bash
# LLM para gera√ß√£o de respostas
ollama pull llama3.1:8b

# LLM multimodal para sumarizar imagens na indexa√ß√£o
ollama pull llava:13b

# Modelo de embeddings (texto)
ollama pull nomic-embed-text

# Certifica que os modelos est√£o instalados
ollama list
```

No **Windows**, para expor o Ollama para a rede/WSL:

```powershell
$env:OLLAMA_HOST="0.0.0.0:11434"
$env:OLLAMA_ORIGINS="*"
ollama serve
# libere a porta 11434 no Firewall do Windows
```


Comando para iniciar a API:

```
uvicorn api:app --reload
```

Como fazer perguntas:

Pergunta unica:
```
curl --request POST \
  --url http://127.0.0.1:8000/ask \
  --header 'Content-Type: application/json' \
  --header 'User-Agent: insomnia/11.5.0' \
  --data '{
	"question": "quantas vezes posso trancar a matricula?"
}'
```

Modo Chat:
```
curl --request POST \
  --url http://127.0.0.1:8000/chat \
  --header 'Content-Type: application/json' \
  --header 'User-Agent: insomnia/11.5.0' \
  --data '{
	"messages": [
	{"role": "user", "content": "Me explique brevemente sobre a matricula"},	
			{"role": "system", "content": "Ol√°!"},
{"role": "user", "content": "Ol√°"}
	]
}'
```

## üó∫Ô∏è Configura√ß√£o de paths

Os paths s√£o absolutos (via `Path.resolve()`) a partir da raiz do reposit√≥rio:

- PDFs: `data_extraction/documentos_ufabc/Prograd`
- Cache de chunks: `.cache_chunks/chunks_classificados.json`
- Cache de summaries: `.cache_chunks/summaries.json`
- Vetores/Chroma + docstore: `.cache_chunks/chroma_store/`

# ü§ñ Chatbot Assistente UFABC (RAG)

Chatbot para a UFABC baseado em **Retrieval-Augmented Generation (RAG)**. Ele busca trechos em documentos institucionais (PDFs, tabelas e imagens), sumariza e usa um LLM para responder com contexto.

## ‚ú® Principais features

- **RAG multimodal**: texto, tabelas (HTML) e imagens (sumarizadas na indexa√ß√£o)
- **Persist√™ncia**: vetores em **Chroma** e **docstore** em disco (LocalFileStore)
- **Reidrata√ß√£o inteligente**: se o docstore sumir, ele √© reconstru√≠do **sem re-embedar**
- **API FastAPI** (concorr√™ncia pronta) + **CLI** (modo terminal)
- **Ollama** para embeddings locais (fallbacks de LLM: Groq/OpenAI, se configurados)
- **Classifica√ß√£o robusta** em `parse_docs` (evita confundir texto com base64)

## üóÇÔ∏è Estrutura do projeto

```
rag_pipeline/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ api.py                     # FastAPI (servi√ßo)
‚îú‚îÄ‚îÄ main.py                    # CLI (terminal)
‚îú‚îÄ‚îÄ config.py                  # Paths absolutos e configs
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # get_llama_model / get_llava_model
‚îÇ   ‚îú‚îÄ‚îÄ prompt_utils.py        # parse_docs, build_prompt etc.
‚îÇ   ‚îî‚îÄ‚îÄ retriever_pipeline.py  # get_rag_pipeline (Op√ß√£o B com reidrata√ß√£o)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ pdf_utils.py           # extra√ß√£o (unstructured) e classifica√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ summarization.py       # sumariza√ß√£o + add_documents (vectorstore + docstore)
‚îÇ   ‚îî‚îÄ‚îÄ retry.py               # retry_with_backoff
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ display_utils.py       # helper p/ exibir imagens base64 (CLI)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ .cache_chunks/             # gerado em runtime (chroma_store, summaries, chunks)
```

## üß© Requisitos

- Python **3.10+**
- [Ollama](https://ollama.com/download) (para embeddings locais)
- (Opcional) chaves **OpenAI** / **Groq** para fallback do LLM

### Instala√ß√£o e execu√ß√£o do back-end python

0. **Entre na past back-end**:
   ```bash
   cd back-end
   ```

1. **Configure o ambiente Python**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate     # Linux/macOS
   # .venv\Scripts\activate      # Windows
   
   cd back-end

   pip install -r requirements.txt
   ```

2. **Configure as vari√°veis de ambiente**:
   ```bash
   cp .env.example .env
   # Edite o arquivo .env com suas chaves de API
   ```

3. **Verifique se o Ollama est√° rodando**:
   ```bash
   ollama serve
   ```

4. **Se for usar modelos locais, instale os modelos**:
   ```bash
   ollama pull llama3.2:latest
   ollama pull nomic-embed-text:latest
   ```

5. **Rode a aplica√ß√£o** (escolha uma das op√ß√µes):

   ### üåê **API FastAPI** (Servidor Web)
   
   **Op√ß√£o 1 - Como m√≥dulo (Recomendado):**
   ```bash
   cd src
   python3 -m rag_pipeline.api
   ```
   
   **Op√ß√£o 2 - Do diret√≥rio back-end:**
   ```bash
   # A partir do diret√≥rio back-end
   python3 -m src.rag_pipeline.api
   ```
   
   **Op√ß√£o 3 - Com PYTHONPATH:**
   ```bash
   PYTHONPATH=src python3 src/rag_pipeline/api.py
   ```

   ### üíª **CLI (Terminal Interativo)**
   
   **Op√ß√£o 1 - Como m√≥dulo (Recomendado):**
   ```bash
   cd src
   python3 -m rag_pipeline.main
   ```
   
   **Op√ß√£o 2 - Do diret√≥rio back-end:**
   ```bash
   # A partir do diret√≥rio back-end
   python3 -m src.rag_pipeline.main
   ```
   
   **Op√ß√£o 3 - Com PYTHONPATH:**
   ```bash
   PYTHONPATH=src python3 src/rag_pipeline/main.py
   ```

   ### üîç **Diferen√ßas entre API e CLI:**
   
   - **API (api.py)**: 
     - Servidor web FastAPI rodando na porta 8000
     - Aceita requisi√ß√µes HTTP (POST /ask, POST /chat, GET /health)
     - Ideal para integra√ß√£o com front-end ou outras aplica√ß√µes
     - Carrega o modelo uma vez e reutiliza entre requisi√ß√µes
   
   - **CLI (main.py)**:
     - Interface de terminal interativa
     - Permite fazer perguntas diretamente no terminal
     - Ideal para testes r√°pidos e desenvolvimento
     - Pergunta se deve regenerar os chunks na inicializa√ß√£o
     - Digite 'sair' para encerrar a sess√£o

   ### üìù **Exemplo de uso do CLI:**
   ```bash
   cd src
   python3 -m rag_pipeline.main
   
   # Sa√≠da esperada:
   # üîÑ Deseja gerar os chunks novamente? (s/n): n
   # Usando cache de chunks existente...
   # Pergunta (ou 'sair'): O que √© o PGC?
   # [Resposta do modelo]
   # Pergunta (ou 'sair'): sair
   ```

> **Importante**: Use uma das op√ß√µes acima para evitar erros de import relativo. N√£o execute diretamente `python3 src/rag_pipeline/api.py` pois isso causar√° `ImportError: attempted relative import with no known parent package`.

> **Nota**: Se o `unstructured.partition.pdf` pedir extras (OCR), instale variantes como `unstructured[all-docs]`.

## üîê Configura√ß√£o de Vari√°veis de Ambiente

### Setup para Desenvolvimento

1. **Copie o arquivo de exemplo**:
   ```bash
   cp .env.example .env
   ```

2. **Edite o arquivo `.env`** com suas chaves reais:
   ```bash
   # API Keys obrigat√≥rias
   GROQ_API_KEY=sua_chave_groq_aqui
   OPENAI_API_KEY=sua_chave_openai_aqui
   
   # Base URL do Ollama (padr√£o: localhost)
   OLLAMA_BASE_URL=http://localhost:11434
   
   # Provedor de modelo para gera√ß√£o de texto (padr√£o: ollama)
   # Op√ß√µes: ollama, openai, groq
   MODEL_PROVIDER=ollama
   
   # Provedor de embeddings (padr√£o: ollama)
   # Op√ß√µes: ollama, nomic
   EMBEDDINGS_PROVIDER=ollama
   
   # Chave da API Nomic (necess√°ria apenas se EMBEDDINGS_PROVIDER=nomic)
   NOMIC_KEY=sua_chave_nomic_aqui
   ```

### Obtendo as API Keys

- **Groq**: Registre-se em [console.groq.com](https://console.groq.com) para obter sua chave gratuita
- **OpenAI**: Acesse [platform.openai.com/api-keys](https://platform.openai.com/api-keys) para gerar uma API key
- **Nomic**: Registre-se em [atlas.nomic.ai](https://atlas.nomic.ai) para obter sua chave de API (necess√°ria apenas se usar `EMBEDDINGS_PROVIDER=nomic`)

### Configura√ß√£o para Produ√ß√£o/Servidor

Em ambiente de produ√ß√£o, defina as vari√°veis de ambiente diretamente no sistema:

```bash
# Linux/macOS
export GROQ_API_KEY="sua_chave_groq"
export OPENAI_API_KEY="sua_chave_openai"
export OLLAMA_BASE_URL="http://seu-servidor-ollama:11434"
export MODEL_PROVIDER="ollama"
export EMBEDDINGS_PROVIDER="ollama"
export NOMIC_KEY="sua_chave_nomic"

# Windows
set GROQ_API_KEY=sua_chave_groq
set OPENAI_API_KEY=sua_chave_openai
set OLLAMA_BASE_URL=http://seu-servidor-ollama:11434
set MODEL_PROVIDER=ollama
set EMBEDDINGS_PROVIDER=ollama
set NOMIC_KEY=sua_chave_nomic
```

### Valida√ß√£o das Vari√°veis

O sistema valida automaticamente se todas as vari√°veis obrigat√≥rias est√£o configuradas:
- `GROQ_API_KEY`
- `OPENAI_API_KEY` 
- `OLLAMA_BASE_URL`
- `MODEL_PROVIDER` (opcional, padr√£o: "ollama")
- `EMBEDDINGS_PROVIDER` (opcional, padr√£o: "ollama")
- `NOMIC_KEY` (obrigat√≥ria apenas se `EMBEDDINGS_PROVIDER=nomic`)

Se alguma vari√°vel estiver faltando, voc√™ ver√° um erro como:
```
ValueError: Missing required environment variables: GROQ_API_KEY, OLLAMA_BASE_URL
```

### Configura√ß√£o de Provedores

#### Provedores de Modelos de Texto (MODEL_PROVIDER)
- **ollama** (padr√£o): Usa modelo local `llama3.1:8b`
- **openai**: Usa `gpt-4o-mini` da OpenAI
- **groq**: Usa `llama-3.1-8b-instant` da Groq

#### Provedores de Embeddings (EMBEDDINGS_PROVIDER)
- **ollama** (padr√£o): Usa modelo local `nomic-embed-text` via Ollama
- **nomic**: Usa API Nomic `nomic-embed-text-v1.5` (requer `NOMIC_KEY`)

**Vantagens de usar Nomic API para embeddings:**
- N√£o requer instala√ß√£o local do modelo
- Embeddings otimizados e mais r√°pidos
- Ideal para ambientes serverless (AWS Lambda, etc.)
- Melhor performance em produ√ß√£o

## üì¶ Baixar os modelos no Ollama

Certifique-se de que o Ollama est√° rodando (ollama serve) e ent√£o baixe os modelos usados pelo projeto:
```bash
# LLM para gera√ß√£o de respostas
ollama pull llama3.1:8b

# LLM multimodal para sumarizar imagens na indexa√ß√£o
ollama pull llava:13b

# Modelo de embeddings (texto)
ollama pull nomic-embed-text

# Certifica que os modelos est√£o instalados
ollama list
```

No **Windows**, para expor o Ollama para a rede/WSL:

```powershell
$env:OLLAMA_HOST="0.0.0.0:11434"
$env:OLLAMA_ORIGINS="*"
ollama serve
# libere a porta 11434 no Firewall do Windows
```


Como fazer perguntas:

Pergunta unica:
```
curl --request POST \
  --url http://127.0.0.1:8000/ask \
  --header 'Content-Type: application/json' \
  --header 'User-Agent: insomnia/11.5.0' \
  --data '{
	"question": "quantas vezes posso trancar a matricula?"
}'
```

Modo Chat:
```
curl --request POST \
  --url http://127.0.0.1:8000/chat \
  --header 'Content-Type: application/json' \
  --header 'User-Agent: insomnia/11.5.0' \
  --data '{
	"messages": [
	{"role": "user", "content": "Me explique brevemente sobre a matricula"},	
			{"role": "system", "content": "Ol√°!"},
{"role": "user", "content": "Ol√°"}
	]
}'
```

## üó∫Ô∏è Configura√ß√£o de paths

Os paths s√£o absolutos (via `Path.resolve()`) a partir da raiz do reposit√≥rio:

- PDFs: `data_extraction/documentos_ufabc/Prograd`
- Cache de chunks: `.cache_chunks/chunks_classificados.json`
- Cache de summaries: `.cache_chunks/summaries.json`
- Vetores/Chroma + docstore: `.cache_chunks/chroma_store/`


docker build --platform linux/amd64 -t aws_rag_app .

# Option 1: Use host network (recommended for full host access on Linux)
docker run --rm --network host \
--entrypoint python \
--env-file .env \
aws_rag_app rag_pipeline/api.py

# Option 2: Use bridge mode with host gateway (for macOS/Windows or port mapping)
docker run --rm -p 8000:8000 \
--add-host=host.docker.internal:host-gateway \
--entrypoint python \
--env-file .env \
aws_rag_app rag_pipeline/api.py

## ‚òÅÔ∏è Deploy para AWS Lambda (CDK)

### Pr√©-requisitos

1. **AWS CLI configurado**:
   ```bash
   aws configure
   # Insira suas credenciais AWS (Access Key ID, Secret Access Key, regi√£o)
   ```

2. **AWS CDK instalado**:
   ```bash
   npm install -g aws-cdk
   ```

3. **Node.js** (vers√£o 18+) instalado

### Deploy

1. **Entre no diret√≥rio CDK**:
   ```bash
   cd rag-cdk-infra
   ```

2. **Instale as depend√™ncias**:
   ```bash
   npm install
   ```

3. **Configure o Bootstrap CDK** (apenas na primeira vez):
   ```bash
   cdk bootstrap
   ```

4. **Deploy da aplica√ß√£o**:
   ```bash
   cdk deploy
   ```

5. **Ap√≥s o deploy**, voc√™ receber√° uma URL como:
   ```
   ‚úÖ RagCdkInfraStack
   Outputs:
   RagCdkInfraStack.FunctionUrl = https://sua-funcao-id.lambda-url.sa-east-1.on.aws/health
   ```

### Testando o Deploy

```bash
# Teste o endpoint de sa√∫de
curl https://sua-funcao-id.lambda-url.sa-east-1.on.aws/health

# Teste uma pergunta
curl -X POST "https://sua-funcao-id.lambda-url.sa-east-1.on.aws/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "O que √© o PGC?"}'
```

### Configura√ß√£o do Ambiente

O CDK carrega automaticamente as vari√°veis de ambiente do arquivo `back-end/.env`. Certifique-se de que todas as vari√°veis necess√°rias est√£o configuradas:

- `GROQ_API_KEY` (obrigat√≥ria)
- `OPENAI_API_KEY` (opcional)
- `NOMIC_KEY` (obrigat√≥ria se usando embeddings Nomic)
- `MODEL_PROVIDER` (padr√£o: groq)
- `EMBEDDINGS_PROVIDER` (padr√£o: nomic)

## üìä Verificando Logs do Lambda

### M√©todo R√°pido (Script Automatizado)

Use o script fornecido no diret√≥rio CDK:

```bash
cd rag-cdk-infra
./check-logs.sh
```

### M√©todo Manual (AWS CLI)

1. **Listar os log streams mais recentes**:
   ```bash
   aws logs describe-log-streams \
     --log-group-name "/aws/lambda/RagCdkInfraStack-ApiFunc9527395A-CbVbFfQfMSzf" \
     --order-by LastEventTime --descending --max-items 3
   ```

2. **Obter logs de um stream espec√≠fico**:
   ```bash
   # Substitua STREAM_NAME pelo nome obtido no comando anterior
   aws logs get-log-events \
     --log-group-name "/aws/lambda/RagCdkInfraStack-ApiFunc9527395A-CbVbFfQfMSzf" \
     --log-stream-name "STREAM_NAME" \
     --start-time $(date -d '10 minutes ago' +%s)000
   ```

### Logs em Tempo Real

Para monitorar logs em tempo real durante testes:

```bash
# Instalar ferramenta de streaming de logs (opcional)
npm install -g aws-logs-cli

# Stream logs em tempo real
aws-logs /aws/lambda/RagCdkInfraStack-ApiFunc9527395A-CbVbFfQfMSzf
```

### Limpeza

Para remover todos os recursos AWS criados:

```bash
cd rag-cdk-infra
cdk destroy
```

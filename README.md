# ğŸ¤– Chatbot Assistente UFABC

Este repositÃ³rio contÃ©m o desenvolvimento de um **chatbot assistente para a UFABC**, utilizando a arquitetura de **Retrieval-Augmented Generation (RAG)**. O objetivo do projeto Ã© facilitar o acesso a informaÃ§Ãµes institucionais, documentos e dÃºvidas frequentes da universidade de forma automatizada, eficiente e interativa.

## ğŸ’¡ Sobre o Projeto

O chatbot tem como propÃ³sito atender Ã  comunidade acadÃªmica da UFABC (Universidade Federal do ABC), oferecendo respostas contextualmente relevantes a partir de uma base de documentos institucionais, como editais, regulamentos e informaÃ§Ãµes acadÃªmicas.

A abordagem RAG combina tÃ©cnicas de recuperaÃ§Ã£o de documentos com geraÃ§Ã£o de linguagem natural, permitindo que o chatbot consulte documentos reais antes de formular respostas, garantindo maior precisÃ£o e confiabilidade.

## âš™ï¸ Tecnologias e Conceitos

- **Retrieval-Augmented Generation (RAG)**
- **Processamento de Linguagem Natural (PLN)**

## ğŸ‘¥ Autores

- **Aline Milene Martins dos Santos**  
  ğŸ“§ aline.milene@aluno.ufabc.edu.br  
  ğŸ”— [github.com/AlineMilene](https://github.com/AlineMilene)

- **Leonardo Pires de Oliveira**  
  ğŸ“§ oliveira.l@aluno.ufabc.edu.br  
  ğŸ”— [github.com/LeonOliveir4](https://github.com/LeonOliveir4)

- **Matheus Victor Soares de Araujo**  
  ğŸ“§ matheus.victor@aluno.ufabc.edu.br  
  ğŸ”— [github.com/MatheusR42](https://github.com/MatheusR42)

---

Este projeto faz parte do Projeto de GraduaÃ§Ã£o de Curso (PGC) na **Universidade Federal do ABC (UFABC)**.

## ğŸš€ Como executar o projeto localmente

1ï¸âƒ£ Crie e ative um ambiente virtual (opcional, mas recomendado):

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/macOS
.venv\Scripts\activate      # Windows
```

2ï¸âƒ£ Instale as dependÃªncias:

```bash
pip install bm25s[full] fastapi uvicorn openai
uvicorn rag_pipeline.main:app --host 0.0.0.0 --port 8000 --reload
```

3ï¸âƒ£ Para uso com LLaMA local (vLLM)

```bash
pip install vllm
python -m vllm.entrypoints.openai.api_server --model meta-llama/Meta-Llama-3-8B-Instruct
```
#### Configure o endpoint da LLM no arquivo llama_client.py para:
```bash
openai.base_url = "http://localhost:8000/v1"
```

### ğŸ”‘ Acesso ao modelo LLaMA 3.1 (obrigatÃ³rio)

Para usar o modelo `meta-llama/Llama-3.1-8B-Instruct` com vLLM, vocÃª precisa:

1ï¸âƒ£ Ter uma conta no [Hugging Face](https://huggingface.co)

2ï¸âƒ£ Solicitar acesso ao modelo na pÃ¡gina:  
[https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)

3ï¸âƒ£ ApÃ³s aprovaÃ§Ã£o (leva de 1 a 5 dias), gerar um Access Token em:  
[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)  
ğŸ‘‰ Selecione permissÃ£o `Read` ao criar o token.

4ï¸âƒ£ Logar na sua mÃ¡quina usando o token:
```bash
pip install huggingface-hub
huggingface-cli login
```

### â— Importante: se desejar testar o pipeline antes da aprovaÃ§Ã£o, utilize o modelo open-source:

```bash
python -m vllm.entrypoints.openai.api_server --model mistralai/Mistral-7B-Instruct-v0.2
```